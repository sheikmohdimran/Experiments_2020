{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRC_Transformer_generator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheikmohdimran/Experiments_2020/blob/master/NLP/GRC_Transformer_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knk1cw4NL7MZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "765ad08a-5427-4db5-db83-bc1ecbaeed12"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 17364 (delta 5), reused 12 (delta 3), pack-reused 17349\u001b[K\n",
            "Receiving objects: 100% (17364/17364), 10.11 MiB | 6.15 MiB/s, done.\n",
            "Resolving deltas: 100% (12925/12925), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGWaoue-N6ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab7615a6-0953-4072-ad9c-b1cdbf2f4a4d"
      },
      "source": [
        "!cd /content/transformers\n",
        "!pip install /content/transformers\n",
        "!pip install -r /content/transformers/examples/requirements.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.17.4)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.10.40)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.9)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 48.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 59.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (1.13.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.2.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers==2.3.0) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers==2.3.0) (0.15.2)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.3.0-cp36-none-any.whl size=458434 sha256=acc189b40963b573a668dccffb70adb858be0362a4a6466c1da1a326428fb8c8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zrpxw_e5/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=f6500289ca210af1e47de3a6f5f2aadb716cea818a52e8e9fe01f827d798592a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.3.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r /content/transformers/examples/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r /content/transformers/examples/requirements.txt (line 3)) (0.21.3)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r /content/transformers/examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r /content/transformers/examples/requirements.txt (line 1)) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r /content/transformers/examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/transformers/examples/requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/transformers/examples/requirements.txt (line 2)) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/transformers/examples/requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/transformers/examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/transformers/examples/requirements.txt (line 2)) (42.0.2)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/transformers/examples/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r /content/transformers/examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r /content/transformers/examples/requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r /content/transformers/examples/requirements.txt (line 4)) (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r /content/transformers/examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r /content/transformers/examples/requirements.txt (line 4)) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r /content/transformers/examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r /content/transformers/examples/requirements.txt (line 4)) (1.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=79b42e36a23d8708f648769e8bccb4efde9b0650b6e3a616ff82e80fb6273304\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tensorboardX, seqeval\n",
            "Successfully installed seqeval-0.0.12 tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny00Ppd6MR31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14672e41-f0f8-4dcb-ad67-c65ea0413378"
      },
      "source": [
        "!python /content/transformers/examples/run_lm_finetuning.py\\\n",
        "    --output_dir=output\\\n",
        "    --model_type=gpt2\\\n",
        "    --model_name_or_path=gpt2\\\n",
        "    --do_train\\\n",
        "    --train_data_file=/content/input.txt \\\n",
        "    --per_gpu_train_batch_size 2"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/07/2020 13:36:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/07/2020 13:36:48 - INFO - filelock -   Lock 139682431651736 acquired on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80.lock\n",
            "01/07/2020 13:36:48 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp1s75zav9\n",
            "Downloading: 100% 176/176 [00:00<00:00, 151kB/s]\n",
            "01/07/2020 13:36:49 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
            "01/07/2020 13:36:49 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
            "01/07/2020 13:36:49 - INFO - filelock -   Lock 139682431651736 released on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80.lock\n",
            "01/07/2020 13:36:49 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
            "01/07/2020 13:36:49 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "01/07/2020 13:36:49 - INFO - filelock -   Lock 139682431649328 acquired on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "01/07/2020 13:36:49 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpb26udtai\n",
            "Downloading: 100% 1.04M/1.04M [00:01<00:00, 971kB/s]\n",
            "01/07/2020 13:36:51 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "01/07/2020 13:36:51 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "01/07/2020 13:36:51 - INFO - filelock -   Lock 139682431649328 released on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "01/07/2020 13:36:52 - INFO - filelock -   Lock 139682431650560 acquired on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "01/07/2020 13:36:52 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp7yg1pdbw\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 636kB/s]\n",
            "01/07/2020 13:36:53 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "01/07/2020 13:36:53 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "01/07/2020 13:36:53 - INFO - filelock -   Lock 139682431650560 released on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "01/07/2020 13:36:53 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "01/07/2020 13:36:53 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "01/07/2020 13:36:54 - INFO - filelock -   Lock 139682431648096 acquired on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "01/07/2020 13:36:54 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpzx3ygfis\n",
            "Downloading: 100% 548M/548M [00:45<00:00, 12.0MB/s]\n",
            "01/07/2020 13:37:41 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin in cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "01/07/2020 13:37:41 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "01/07/2020 13:37:41 - INFO - filelock -   Lock 139682431648096 released on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "01/07/2020 13:37:41 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "01/07/2020 13:37:54 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir='', config_name='', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=2, save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', tokenizer_name='', train_data_file='/content/input.txt', warmup_steps=0, weight_decay=0.0)\n",
            "01/07/2020 13:37:54 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "01/07/2020 13:37:57 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_input.txt\n",
            "01/07/2020 13:37:57 - INFO - __main__ -   ***** Running training *****\n",
            "01/07/2020 13:37:57 - INFO - __main__ -     Num examples = 950\n",
            "01/07/2020 13:37:57 - INFO - __main__ -     Num Epochs = 1\n",
            "01/07/2020 13:37:57 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "01/07/2020 13:37:57 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "01/07/2020 13:37:57 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "01/07/2020 13:37:57 - INFO - __main__ -     Total optimization steps = 475\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/475 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/475 [00:00<04:38,  1.70it/s]\u001b[A\n",
            "Iteration:   0% 2/475 [00:00<04:11,  1.88it/s]\u001b[A\n",
            "Iteration:   1% 3/475 [00:01<03:52,  2.03it/s]\u001b[A\n",
            "Iteration:   1% 4/475 [00:01<03:40,  2.14it/s]\u001b[A\n",
            "Iteration:   1% 5/475 [00:02<03:30,  2.23it/s]\u001b[A\n",
            "Iteration:   1% 6/475 [00:02<03:25,  2.29it/s]\u001b[A\n",
            "Iteration:   1% 7/475 [00:03<03:19,  2.35it/s]\u001b[A\n",
            "Iteration:   2% 8/475 [00:03<03:15,  2.38it/s]\u001b[A\n",
            "Iteration:   2% 9/475 [00:03<03:13,  2.41it/s]\u001b[A\n",
            "Iteration:   2% 10/475 [00:04<03:11,  2.43it/s]\u001b[A\n",
            "Iteration:   2% 11/475 [00:04<03:10,  2.44it/s]\u001b[A\n",
            "Iteration:   3% 12/475 [00:05<03:09,  2.45it/s]\u001b[A\n",
            "Iteration:   3% 13/475 [00:05<03:08,  2.45it/s]\u001b[A\n",
            "Iteration:   3% 14/475 [00:05<03:07,  2.46it/s]\u001b[A\n",
            "Iteration:   3% 15/475 [00:06<03:06,  2.46it/s]\u001b[A\n",
            "Iteration:   3% 16/475 [00:06<03:05,  2.47it/s]\u001b[A\n",
            "Iteration:   4% 17/475 [00:07<03:05,  2.47it/s]\u001b[A\n",
            "Iteration:   4% 18/475 [00:07<03:04,  2.47it/s]\u001b[A\n",
            "Iteration:   4% 19/475 [00:07<03:04,  2.47it/s]\u001b[A\n",
            "Iteration:   4% 20/475 [00:08<03:04,  2.47it/s]\u001b[A\n",
            "Iteration:   4% 21/475 [00:08<03:03,  2.47it/s]\u001b[A\n",
            "Iteration:   5% 22/475 [00:09<03:03,  2.47it/s]\u001b[A\n",
            "Iteration:   5% 23/475 [00:09<03:02,  2.47it/s]\u001b[A\n",
            "Iteration:   5% 24/475 [00:09<03:03,  2.46it/s]\u001b[A\n",
            "Iteration:   5% 25/475 [00:10<03:01,  2.47it/s]\u001b[A\n",
            "Iteration:   5% 26/475 [00:10<03:01,  2.47it/s]\u001b[A\n",
            "Iteration:   6% 27/475 [00:11<03:01,  2.47it/s]\u001b[A\n",
            "Iteration:   6% 28/475 [00:11<03:00,  2.47it/s]\u001b[A\n",
            "Iteration:   6% 29/475 [00:11<03:00,  2.47it/s]\u001b[A\n",
            "Iteration:   6% 30/475 [00:12<03:00,  2.46it/s]\u001b[A\n",
            "Iteration:   7% 31/475 [00:12<02:59,  2.47it/s]\u001b[A\n",
            "Iteration:   7% 32/475 [00:13<03:00,  2.46it/s]\u001b[A\n",
            "Iteration:   7% 33/475 [00:13<02:58,  2.47it/s]\u001b[A\n",
            "Iteration:   7% 34/475 [00:13<02:58,  2.47it/s]\u001b[A\n",
            "Iteration:   7% 35/475 [00:14<02:58,  2.47it/s]\u001b[A\n",
            "Iteration:   8% 36/475 [00:14<02:57,  2.47it/s]\u001b[A\n",
            "Iteration:   8% 37/475 [00:15<02:57,  2.47it/s]\u001b[A\n",
            "Iteration:   8% 38/475 [00:15<02:56,  2.47it/s]\u001b[A\n",
            "Iteration:   8% 39/475 [00:15<02:56,  2.47it/s]\u001b[A\n",
            "Iteration:   8% 40/475 [00:16<02:56,  2.46it/s]\u001b[A\n",
            "Iteration:   9% 41/475 [00:16<02:55,  2.47it/s]\u001b[A\n",
            "Iteration:   9% 42/475 [00:17<02:55,  2.47it/s]\u001b[A\n",
            "Iteration:   9% 43/475 [00:17<02:54,  2.47it/s]\u001b[A\n",
            "Iteration:   9% 44/475 [00:17<02:54,  2.47it/s]\u001b[A\n",
            "Iteration:   9% 45/475 [00:18<02:54,  2.47it/s]\u001b[A\n",
            "Iteration:  10% 46/475 [00:18<02:53,  2.47it/s]\u001b[A\n",
            "Iteration:  10% 47/475 [00:19<02:53,  2.47it/s]\u001b[A\n",
            "Iteration:  10% 48/475 [00:19<02:52,  2.47it/s]\u001b[A\n",
            "Iteration:  10% 49/475 [00:20<02:52,  2.47it/s]\u001b[A01/07/2020 13:38:17 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-50/config.json\n",
            "01/07/2020 13:38:19 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-50/pytorch_model.bin\n",
            "01/07/2020 13:38:19 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-50\n",
            "01/07/2020 13:38:26 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-50\n",
            "\n",
            "Iteration:  11% 50/475 [00:29<21:21,  3.01s/it]\u001b[A\n",
            "Iteration:  11% 51/475 [00:29<15:46,  2.23s/it]\u001b[A\n",
            "Iteration:  11% 52/475 [00:29<11:52,  1.68s/it]\u001b[A\n",
            "Iteration:  11% 53/475 [00:30<09:09,  1.30s/it]\u001b[A\n",
            "Iteration:  11% 54/475 [00:30<07:13,  1.03s/it]\u001b[A\n",
            "Iteration:  12% 55/475 [00:31<05:54,  1.19it/s]\u001b[A\n",
            "Iteration:  12% 56/475 [00:31<04:58,  1.41it/s]\u001b[A\n",
            "Iteration:  12% 57/475 [00:31<04:18,  1.61it/s]\u001b[A\n",
            "Iteration:  12% 58/475 [00:32<03:52,  1.79it/s]\u001b[A\n",
            "Iteration:  12% 59/475 [00:32<03:32,  1.96it/s]\u001b[A\n",
            "Iteration:  13% 60/475 [00:33<03:20,  2.07it/s]\u001b[A\n",
            "Iteration:  13% 61/475 [00:33<03:09,  2.18it/s]\u001b[A\n",
            "Iteration:  13% 62/475 [00:33<03:02,  2.26it/s]\u001b[A\n",
            "Iteration:  13% 63/475 [00:34<02:58,  2.31it/s]\u001b[A\n",
            "Iteration:  13% 64/475 [00:34<02:53,  2.36it/s]\u001b[A\n",
            "Iteration:  14% 65/475 [00:35<02:51,  2.39it/s]\u001b[A\n",
            "Iteration:  14% 66/475 [00:35<02:49,  2.42it/s]\u001b[A\n",
            "Iteration:  14% 67/475 [00:36<02:47,  2.43it/s]\u001b[A\n",
            "Iteration:  14% 68/475 [00:36<02:47,  2.43it/s]\u001b[A\n",
            "Iteration:  15% 69/475 [00:36<02:45,  2.45it/s]\u001b[A\n",
            "Iteration:  15% 70/475 [00:37<02:44,  2.46it/s]\u001b[A\n",
            "Iteration:  15% 71/475 [00:37<02:44,  2.46it/s]\u001b[A\n",
            "Iteration:  15% 72/475 [00:38<02:43,  2.46it/s]\u001b[A\n",
            "Iteration:  15% 73/475 [00:38<02:43,  2.46it/s]\u001b[A\n",
            "Iteration:  16% 74/475 [00:38<02:44,  2.44it/s]\u001b[A\n",
            "Iteration:  16% 75/475 [00:39<02:42,  2.46it/s]\u001b[A\n",
            "Iteration:  16% 76/475 [00:39<02:41,  2.47it/s]\u001b[A\n",
            "Iteration:  16% 77/475 [00:40<02:41,  2.47it/s]\u001b[A\n",
            "Iteration:  16% 78/475 [00:40<02:41,  2.46it/s]\u001b[A\n",
            "Iteration:  17% 79/475 [00:40<02:40,  2.46it/s]\u001b[A\n",
            "Iteration:  17% 80/475 [00:41<02:39,  2.47it/s]\u001b[A\n",
            "Iteration:  17% 81/475 [00:41<02:39,  2.47it/s]\u001b[A\n",
            "Iteration:  17% 82/475 [00:42<02:39,  2.47it/s]\u001b[A\n",
            "Iteration:  17% 83/475 [00:42<02:39,  2.46it/s]\u001b[A\n",
            "Iteration:  18% 84/475 [00:42<02:38,  2.47it/s]\u001b[A\n",
            "Iteration:  18% 85/475 [00:43<02:37,  2.47it/s]\u001b[A\n",
            "Iteration:  18% 86/475 [00:43<02:37,  2.47it/s]\u001b[A\n",
            "Iteration:  18% 87/475 [00:44<02:36,  2.47it/s]\u001b[A\n",
            "Iteration:  19% 88/475 [00:44<02:36,  2.47it/s]\u001b[A\n",
            "Iteration:  19% 89/475 [00:44<02:36,  2.46it/s]\u001b[A\n",
            "Iteration:  19% 90/475 [00:45<02:35,  2.47it/s]\u001b[A\n",
            "Iteration:  19% 91/475 [00:45<02:35,  2.47it/s]\u001b[A\n",
            "Iteration:  19% 92/475 [00:46<02:34,  2.47it/s]\u001b[A\n",
            "Iteration:  20% 93/475 [00:46<02:34,  2.47it/s]\u001b[A\n",
            "Iteration:  20% 94/475 [00:46<02:35,  2.46it/s]\u001b[A\n",
            "Iteration:  20% 95/475 [00:47<02:33,  2.47it/s]\u001b[A\n",
            "Iteration:  20% 96/475 [00:47<02:33,  2.47it/s]\u001b[A\n",
            "Iteration:  20% 97/475 [00:48<02:32,  2.47it/s]\u001b[A\n",
            "Iteration:  21% 98/475 [00:48<02:32,  2.47it/s]\u001b[A\n",
            "Iteration:  21% 99/475 [00:48<02:32,  2.47it/s]\u001b[A01/07/2020 13:38:46 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-100/config.json\n",
            "01/07/2020 13:38:48 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-100/pytorch_model.bin\n",
            "01/07/2020 13:38:48 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-100\n",
            "01/07/2020 13:39:01 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-100\n",
            "\n",
            "Iteration:  21% 100/475 [01:04<31:08,  4.98s/it]\u001b[A\n",
            "Iteration:  21% 101/475 [01:05<22:30,  3.61s/it]\u001b[A\n",
            "Iteration:  21% 102/475 [01:05<16:27,  2.65s/it]\u001b[A\n",
            "Iteration:  22% 103/475 [01:05<12:14,  1.97s/it]\u001b[A\n",
            "Iteration:  22% 104/475 [01:06<09:17,  1.50s/it]\u001b[A\n",
            "Iteration:  22% 105/475 [01:06<07:14,  1.17s/it]\u001b[A\n",
            "Iteration:  22% 106/475 [01:07<05:48,  1.06it/s]\u001b[A\n",
            "Iteration:  23% 107/475 [01:07<04:47,  1.28it/s]\u001b[A\n",
            "Iteration:  23% 108/475 [01:07<04:05,  1.50it/s]\u001b[A\n",
            "Iteration:  23% 109/475 [01:08<03:35,  1.70it/s]\u001b[A\n",
            "Iteration:  23% 110/475 [01:08<03:14,  1.87it/s]\u001b[A\n",
            "Iteration:  23% 111/475 [01:09<03:00,  2.02it/s]\u001b[A\n",
            "Iteration:  24% 112/475 [01:09<02:49,  2.14it/s]\u001b[A\n",
            "Iteration:  24% 113/475 [01:09<02:42,  2.23it/s]\u001b[A\n",
            "Iteration:  24% 114/475 [01:10<02:37,  2.30it/s]\u001b[A\n",
            "Iteration:  24% 115/475 [01:10<02:33,  2.35it/s]\u001b[A\n",
            "Iteration:  24% 116/475 [01:11<02:30,  2.38it/s]\u001b[A\n",
            "Iteration:  25% 117/475 [01:11<02:28,  2.41it/s]\u001b[A\n",
            "Iteration:  25% 118/475 [01:11<02:26,  2.43it/s]\u001b[A\n",
            "Iteration:  25% 119/475 [01:12<02:25,  2.44it/s]\u001b[A\n",
            "Iteration:  25% 120/475 [01:12<02:24,  2.45it/s]\u001b[A\n",
            "Iteration:  25% 121/475 [01:13<02:25,  2.44it/s]\u001b[A\n",
            "Iteration:  26% 122/475 [01:13<02:23,  2.45it/s]\u001b[A\n",
            "Iteration:  26% 123/475 [01:13<02:22,  2.46it/s]\u001b[A\n",
            "Iteration:  26% 124/475 [01:14<02:22,  2.47it/s]\u001b[A\n",
            "Iteration:  26% 125/475 [01:14<02:21,  2.47it/s]\u001b[A\n",
            "Iteration:  27% 126/475 [01:15<02:21,  2.47it/s]\u001b[A\n",
            "Iteration:  27% 127/475 [01:15<02:21,  2.46it/s]\u001b[A\n",
            "Iteration:  27% 128/475 [01:15<02:20,  2.47it/s]\u001b[A\n",
            "Iteration:  27% 129/475 [01:16<02:20,  2.47it/s]\u001b[A\n",
            "Iteration:  27% 130/475 [01:16<02:19,  2.47it/s]\u001b[A\n",
            "Iteration:  28% 131/475 [01:17<02:19,  2.47it/s]\u001b[A\n",
            "Iteration:  28% 132/475 [01:17<02:18,  2.47it/s]\u001b[A\n",
            "Iteration:  28% 133/475 [01:18<02:18,  2.46it/s]\u001b[A\n",
            "Iteration:  28% 134/475 [01:18<02:17,  2.47it/s]\u001b[A\n",
            "Iteration:  28% 135/475 [01:18<02:17,  2.47it/s]\u001b[A\n",
            "Iteration:  29% 136/475 [01:19<02:17,  2.47it/s]\u001b[A\n",
            "Iteration:  29% 137/475 [01:19<02:16,  2.47it/s]\u001b[A\n",
            "Iteration:  29% 138/475 [01:20<02:16,  2.47it/s]\u001b[A\n",
            "Iteration:  29% 139/475 [01:20<02:15,  2.47it/s]\u001b[A\n",
            "Iteration:  29% 140/475 [01:20<02:16,  2.46it/s]\u001b[A\n",
            "Iteration:  30% 141/475 [01:21<02:15,  2.47it/s]\u001b[A\n",
            "Iteration:  30% 142/475 [01:21<02:14,  2.47it/s]\u001b[A\n",
            "Iteration:  30% 143/475 [01:22<02:14,  2.47it/s]\u001b[A\n",
            "Iteration:  30% 144/475 [01:22<02:13,  2.47it/s]\u001b[A\n",
            "Iteration:  31% 145/475 [01:22<02:13,  2.47it/s]\u001b[A\n",
            "Iteration:  31% 146/475 [01:23<02:13,  2.47it/s]\u001b[A\n",
            "Iteration:  31% 147/475 [01:23<02:13,  2.46it/s]\u001b[A\n",
            "Iteration:  31% 148/475 [01:24<02:12,  2.48it/s]\u001b[A\n",
            "Iteration:  31% 149/475 [01:24<02:11,  2.47it/s]\u001b[A01/07/2020 13:39:22 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-150/config.json\n",
            "01/07/2020 13:39:23 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-150/pytorch_model.bin\n",
            "01/07/2020 13:39:23 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-150\n",
            "01/07/2020 13:39:41 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-150\n",
            "\n",
            "Iteration:  32% 150/475 [01:43<32:45,  6.05s/it]\u001b[A\n",
            "Iteration:  32% 151/475 [01:44<23:32,  4.36s/it]\u001b[A\n",
            "Iteration:  32% 152/475 [01:44<17:04,  3.17s/it]\u001b[A\n",
            "Iteration:  32% 153/475 [01:44<12:34,  2.34s/it]\u001b[A\n",
            "Iteration:  32% 154/475 [01:45<09:25,  1.76s/it]\u001b[A\n",
            "Iteration:  33% 155/475 [01:45<07:13,  1.35s/it]\u001b[A\n",
            "Iteration:  33% 156/475 [01:46<05:40,  1.07s/it]\u001b[A\n",
            "Iteration:  33% 157/475 [01:46<04:36,  1.15it/s]\u001b[A\n",
            "Iteration:  33% 158/475 [01:46<03:52,  1.36it/s]\u001b[A\n",
            "Iteration:  33% 159/475 [01:47<03:20,  1.58it/s]\u001b[A\n",
            "Iteration:  34% 160/475 [01:47<02:58,  1.77it/s]\u001b[A\n",
            "Iteration:  34% 161/475 [01:48<02:42,  1.93it/s]\u001b[A\n",
            "Iteration:  34% 162/475 [01:48<02:31,  2.07it/s]\u001b[A\n",
            "Iteration:  34% 163/475 [01:48<02:23,  2.17it/s]\u001b[A\n",
            "Iteration:  35% 164/475 [01:49<02:18,  2.24it/s]\u001b[A\n",
            "Iteration:  35% 165/475 [01:49<02:13,  2.32it/s]\u001b[A\n",
            "Iteration:  35% 166/475 [01:50<02:10,  2.36it/s]\u001b[A\n",
            "Iteration:  35% 167/475 [01:50<02:08,  2.39it/s]\u001b[A\n",
            "Iteration:  35% 168/475 [01:50<02:07,  2.42it/s]\u001b[A\n",
            "Iteration:  36% 169/475 [01:51<02:05,  2.43it/s]\u001b[A\n",
            "Iteration:  36% 170/475 [01:51<02:04,  2.44it/s]\u001b[A\n",
            "Iteration:  36% 171/475 [01:52<02:03,  2.45it/s]\u001b[A\n",
            "Iteration:  36% 172/475 [01:52<02:03,  2.46it/s]\u001b[A\n",
            "Iteration:  36% 173/475 [01:53<02:02,  2.46it/s]\u001b[A\n",
            "Iteration:  37% 174/475 [01:53<02:02,  2.47it/s]\u001b[A\n",
            "Iteration:  37% 175/475 [01:53<02:02,  2.46it/s]\u001b[A\n",
            "Iteration:  37% 176/475 [01:54<02:00,  2.47it/s]\u001b[A\n",
            "Iteration:  37% 177/475 [01:54<02:00,  2.47it/s]\u001b[A\n",
            "Iteration:  37% 178/475 [01:55<02:00,  2.47it/s]\u001b[A\n",
            "Iteration:  38% 179/475 [01:55<01:59,  2.47it/s]\u001b[A\n",
            "Iteration:  38% 180/475 [01:55<01:59,  2.47it/s]\u001b[A\n",
            "Iteration:  38% 181/475 [01:56<01:58,  2.47it/s]\u001b[A\n",
            "Iteration:  38% 182/475 [01:56<01:58,  2.47it/s]\u001b[A\n",
            "Iteration:  39% 183/475 [01:57<01:58,  2.47it/s]\u001b[A\n",
            "Iteration:  39% 184/475 [01:57<01:57,  2.47it/s]\u001b[A\n",
            "Iteration:  39% 185/475 [01:57<01:57,  2.47it/s]\u001b[A\n",
            "Iteration:  39% 186/475 [01:58<01:56,  2.47it/s]\u001b[A\n",
            "Iteration:  39% 187/475 [01:58<01:56,  2.47it/s]\u001b[A\n",
            "Iteration:  40% 188/475 [01:59<01:56,  2.47it/s]\u001b[A\n",
            "Iteration:  40% 189/475 [01:59<01:55,  2.47it/s]\u001b[A\n",
            "Iteration:  40% 190/475 [01:59<01:55,  2.47it/s]\u001b[A\n",
            "Iteration:  40% 191/475 [02:00<01:54,  2.47it/s]\u001b[A\n",
            "Iteration:  40% 192/475 [02:00<01:54,  2.47it/s]\u001b[A\n",
            "Iteration:  41% 193/475 [02:01<01:54,  2.47it/s]\u001b[A\n",
            "Iteration:  41% 194/475 [02:01<01:53,  2.47it/s]\u001b[A\n",
            "Iteration:  41% 195/475 [02:01<01:53,  2.47it/s]\u001b[A\n",
            "Iteration:  41% 196/475 [02:02<01:53,  2.46it/s]\u001b[A\n",
            "Iteration:  41% 197/475 [02:02<01:52,  2.47it/s]\u001b[A\n",
            "Iteration:  42% 198/475 [02:03<01:53,  2.45it/s]\u001b[A\n",
            "Iteration:  42% 199/475 [02:03<01:51,  2.47it/s]\u001b[A01/07/2020 13:40:01 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-200/config.json\n",
            "01/07/2020 13:40:02 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-200/pytorch_model.bin\n",
            "01/07/2020 13:40:03 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-200\n",
            "01/07/2020 13:40:19 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-200\n",
            "\n",
            "Iteration:  42% 200/475 [02:22<26:41,  5.83s/it]\u001b[A\n",
            "Iteration:  42% 201/475 [02:22<19:12,  4.21s/it]\u001b[A\n",
            "Iteration:  43% 202/475 [02:22<13:56,  3.06s/it]\u001b[A\n",
            "Iteration:  43% 203/475 [02:23<10:16,  2.27s/it]\u001b[A\n",
            "Iteration:  43% 204/475 [02:23<07:43,  1.71s/it]\u001b[A\n",
            "Iteration:  43% 205/475 [02:24<05:55,  1.32s/it]\u001b[A\n",
            "Iteration:  43% 206/475 [02:24<04:41,  1.05s/it]\u001b[A\n",
            "Iteration:  44% 207/475 [02:24<03:48,  1.17it/s]\u001b[A\n",
            "Iteration:  44% 208/475 [02:25<03:12,  1.39it/s]\u001b[A\n",
            "Iteration:  44% 209/475 [02:25<02:45,  1.60it/s]\u001b[A\n",
            "Iteration:  44% 210/475 [02:26<02:27,  1.79it/s]\u001b[A\n",
            "Iteration:  44% 211/475 [02:26<02:15,  1.94it/s]\u001b[A\n",
            "Iteration:  45% 212/475 [02:26<02:06,  2.08it/s]\u001b[A\n",
            "Iteration:  45% 213/475 [02:27<01:59,  2.19it/s]\u001b[A\n",
            "Iteration:  45% 214/475 [02:27<01:55,  2.25it/s]\u001b[A\n",
            "Iteration:  45% 215/475 [02:28<01:51,  2.32it/s]\u001b[A\n",
            "Iteration:  45% 216/475 [02:28<01:50,  2.35it/s]\u001b[A\n",
            "Iteration:  46% 217/475 [02:28<01:47,  2.40it/s]\u001b[A\n",
            "Iteration:  46% 218/475 [02:29<01:46,  2.42it/s]\u001b[A\n",
            "Iteration:  46% 219/475 [02:29<01:45,  2.43it/s]\u001b[A\n",
            "Iteration:  46% 220/475 [02:30<01:44,  2.44it/s]\u001b[A\n",
            "Iteration:  47% 221/475 [02:30<01:43,  2.45it/s]\u001b[A\n",
            "Iteration:  47% 222/475 [02:30<01:42,  2.46it/s]\u001b[A\n",
            "Iteration:  47% 223/475 [02:31<01:43,  2.44it/s]\u001b[A\n",
            "Iteration:  47% 224/475 [02:31<01:42,  2.46it/s]\u001b[A\n",
            "Iteration:  47% 225/475 [02:32<01:41,  2.47it/s]\u001b[A\n",
            "Iteration:  48% 226/475 [02:32<01:41,  2.46it/s]\u001b[A\n",
            "Iteration:  48% 227/475 [02:32<01:40,  2.47it/s]\u001b[A\n",
            "Iteration:  48% 228/475 [02:33<01:39,  2.47it/s]\u001b[A\n",
            "Iteration:  48% 229/475 [02:33<01:39,  2.47it/s]\u001b[A\n",
            "Iteration:  48% 230/475 [02:34<01:39,  2.47it/s]\u001b[A\n",
            "Iteration:  49% 231/475 [02:34<01:38,  2.47it/s]\u001b[A\n",
            "Iteration:  49% 232/475 [02:35<01:38,  2.47it/s]\u001b[A\n",
            "Iteration:  49% 233/475 [02:35<01:37,  2.47it/s]\u001b[A\n",
            "Iteration:  49% 234/475 [02:35<01:37,  2.47it/s]\u001b[A\n",
            "Iteration:  49% 235/475 [02:36<01:37,  2.46it/s]\u001b[A\n",
            "Iteration:  50% 236/475 [02:36<01:36,  2.47it/s]\u001b[A\n",
            "Iteration:  50% 237/475 [02:37<01:36,  2.47it/s]\u001b[A\n",
            "Iteration:  50% 238/475 [02:37<01:35,  2.47it/s]\u001b[A\n",
            "Iteration:  50% 239/475 [02:37<01:35,  2.47it/s]\u001b[A\n",
            "Iteration:  51% 240/475 [02:38<01:35,  2.47it/s]\u001b[A\n",
            "Iteration:  51% 241/475 [02:38<01:34,  2.47it/s]\u001b[A\n",
            "Iteration:  51% 242/475 [02:39<01:34,  2.46it/s]\u001b[A\n",
            "Iteration:  51% 243/475 [02:39<01:34,  2.46it/s]\u001b[A\n",
            "Iteration:  51% 244/475 [02:39<01:33,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 245/475 [02:40<01:33,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 246/475 [02:40<01:32,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 247/475 [02:41<01:32,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 248/475 [02:41<01:32,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 249/475 [02:41<01:31,  2.47it/s]\u001b[A01/07/2020 13:40:39 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-250/config.json\n",
            "01/07/2020 13:40:41 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-250/pytorch_model.bin\n",
            "01/07/2020 13:40:41 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-250\n",
            "01/07/2020 13:40:58 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-250\n",
            "\n",
            "Iteration:  53% 250/475 [03:01<22:46,  6.07s/it]\u001b[A\n",
            "Iteration:  53% 251/475 [03:01<16:19,  4.37s/it]\u001b[A\n",
            "Iteration:  53% 252/475 [03:01<11:49,  3.18s/it]\u001b[A\n",
            "Iteration:  53% 253/475 [03:02<08:41,  2.35s/it]\u001b[A\n",
            "Iteration:  53% 254/475 [03:02<06:30,  1.77s/it]\u001b[A\n",
            "Iteration:  54% 255/475 [03:03<04:58,  1.36s/it]\u001b[A\n",
            "Iteration:  54% 256/475 [03:03<03:54,  1.07s/it]\u001b[A\n",
            "Iteration:  54% 257/475 [03:04<03:09,  1.15it/s]\u001b[A\n",
            "Iteration:  54% 258/475 [03:04<02:38,  1.37it/s]\u001b[A\n",
            "Iteration:  55% 259/475 [03:04<02:16,  1.58it/s]\u001b[A\n",
            "Iteration:  55% 260/475 [03:05<02:01,  1.77it/s]\u001b[A\n",
            "Iteration:  55% 261/475 [03:05<01:50,  1.94it/s]\u001b[A\n",
            "Iteration:  55% 262/475 [03:06<01:42,  2.07it/s]\u001b[A\n",
            "Iteration:  55% 263/475 [03:06<01:37,  2.16it/s]\u001b[A\n",
            "Iteration:  56% 264/475 [03:06<01:33,  2.26it/s]\u001b[A\n",
            "Iteration:  56% 265/475 [03:07<01:30,  2.31it/s]\u001b[A\n",
            "Iteration:  56% 266/475 [03:07<01:28,  2.36it/s]\u001b[A\n",
            "Iteration:  56% 267/475 [03:08<01:27,  2.39it/s]\u001b[A\n",
            "Iteration:  56% 268/475 [03:08<01:25,  2.41it/s]\u001b[A\n",
            "Iteration:  57% 269/475 [03:08<01:24,  2.43it/s]\u001b[A\n",
            "Iteration:  57% 270/475 [03:09<01:24,  2.44it/s]\u001b[A\n",
            "Iteration:  57% 271/475 [03:09<01:23,  2.45it/s]\u001b[A\n",
            "Iteration:  57% 272/475 [03:10<01:22,  2.45it/s]\u001b[A\n",
            "Iteration:  57% 273/475 [03:10<01:22,  2.46it/s]\u001b[A\n",
            "Iteration:  58% 274/475 [03:10<01:21,  2.47it/s]\u001b[A\n",
            "Iteration:  58% 275/475 [03:11<01:21,  2.45it/s]\u001b[A\n",
            "Iteration:  58% 276/475 [03:11<01:20,  2.47it/s]\u001b[A\n",
            "Iteration:  58% 277/475 [03:12<01:20,  2.47it/s]\u001b[A\n",
            "Iteration:  59% 278/475 [03:12<01:19,  2.47it/s]\u001b[A\n",
            "Iteration:  59% 279/475 [03:12<01:19,  2.47it/s]\u001b[A\n",
            "Iteration:  59% 280/475 [03:13<01:19,  2.45it/s]\u001b[A\n",
            "Iteration:  59% 281/475 [03:13<01:18,  2.46it/s]\u001b[A\n",
            "Iteration:  59% 282/475 [03:14<01:18,  2.46it/s]\u001b[A\n",
            "Iteration:  60% 283/475 [03:14<01:17,  2.47it/s]\u001b[A\n",
            "Iteration:  60% 284/475 [03:14<01:17,  2.46it/s]\u001b[A\n",
            "Iteration:  60% 285/475 [03:15<01:17,  2.47it/s]\u001b[A\n",
            "Iteration:  60% 286/475 [03:15<01:16,  2.47it/s]\u001b[A\n",
            "Iteration:  60% 287/475 [03:16<01:16,  2.47it/s]\u001b[A\n",
            "Iteration:  61% 288/475 [03:16<01:15,  2.47it/s]\u001b[A\n",
            "Iteration:  61% 289/475 [03:16<01:15,  2.47it/s]\u001b[A\n",
            "Iteration:  61% 290/475 [03:17<01:14,  2.47it/s]\u001b[A\n",
            "Iteration:  61% 291/475 [03:17<01:14,  2.47it/s]\u001b[A\n",
            "Iteration:  61% 292/475 [03:18<01:14,  2.47it/s]\u001b[A\n",
            "Iteration:  62% 293/475 [03:18<01:13,  2.47it/s]\u001b[A\n",
            "Iteration:  62% 294/475 [03:19<01:13,  2.47it/s]\u001b[A\n",
            "Iteration:  62% 295/475 [03:19<01:12,  2.47it/s]\u001b[A\n",
            "Iteration:  62% 296/475 [03:19<01:12,  2.46it/s]\u001b[A\n",
            "Iteration:  63% 297/475 [03:20<01:12,  2.47it/s]\u001b[A\n",
            "Iteration:  63% 298/475 [03:20<01:11,  2.47it/s]\u001b[A\n",
            "Iteration:  63% 299/475 [03:21<01:11,  2.47it/s]\u001b[A01/07/2020 13:41:18 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-300/config.json\n",
            "01/07/2020 13:41:20 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-300/pytorch_model.bin\n",
            "01/07/2020 13:41:20 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-300\n",
            "01/07/2020 13:41:36 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-300\n",
            "\n",
            "Iteration:  63% 300/475 [03:39<16:41,  5.72s/it]\u001b[A\n",
            "Iteration:  63% 301/475 [03:39<11:58,  4.13s/it]\u001b[A\n",
            "Iteration:  64% 302/475 [03:39<08:40,  3.01s/it]\u001b[A\n",
            "Iteration:  64% 303/475 [03:40<06:23,  2.23s/it]\u001b[A\n",
            "Iteration:  64% 304/475 [03:40<04:47,  1.68s/it]\u001b[A\n",
            "Iteration:  64% 305/475 [03:41<03:40,  1.30s/it]\u001b[A\n",
            "Iteration:  64% 306/475 [03:41<02:54,  1.03s/it]\u001b[A\n",
            "Iteration:  65% 307/475 [03:42<02:21,  1.19it/s]\u001b[A\n",
            "Iteration:  65% 308/475 [03:42<01:58,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 309/475 [03:42<01:42,  1.61it/s]\u001b[A\n",
            "Iteration:  65% 310/475 [03:43<01:31,  1.80it/s]\u001b[A\n",
            "Iteration:  65% 311/475 [03:43<01:23,  1.96it/s]\u001b[A\n",
            "Iteration:  66% 312/475 [03:44<01:18,  2.09it/s]\u001b[A\n",
            "Iteration:  66% 313/475 [03:44<01:13,  2.19it/s]\u001b[A\n",
            "Iteration:  66% 314/475 [03:44<01:10,  2.27it/s]\u001b[A\n",
            "Iteration:  66% 315/475 [03:45<01:08,  2.33it/s]\u001b[A\n",
            "Iteration:  67% 316/475 [03:45<01:07,  2.36it/s]\u001b[A\n",
            "Iteration:  67% 317/475 [03:46<01:05,  2.40it/s]\u001b[A\n",
            "Iteration:  67% 318/475 [03:46<01:04,  2.42it/s]\u001b[A\n",
            "Iteration:  67% 319/475 [03:46<01:03,  2.44it/s]\u001b[A\n",
            "Iteration:  67% 320/475 [03:47<01:03,  2.44it/s]\u001b[A\n",
            "Iteration:  68% 321/475 [03:47<01:02,  2.45it/s]\u001b[A\n",
            "Iteration:  68% 322/475 [03:48<01:02,  2.45it/s]\u001b[A\n",
            "Iteration:  68% 323/475 [03:48<01:01,  2.46it/s]\u001b[A\n",
            "Iteration:  68% 324/475 [03:48<01:01,  2.47it/s]\u001b[A\n",
            "Iteration:  68% 325/475 [03:49<01:00,  2.47it/s]\u001b[A\n",
            "Iteration:  69% 326/475 [03:49<01:00,  2.47it/s]\u001b[A\n",
            "Iteration:  69% 327/475 [03:50<00:59,  2.47it/s]\u001b[A\n",
            "Iteration:  69% 328/475 [03:50<00:59,  2.46it/s]\u001b[A\n",
            "Iteration:  69% 329/475 [03:50<00:59,  2.47it/s]\u001b[A\n",
            "Iteration:  69% 330/475 [03:51<00:58,  2.46it/s]\u001b[A\n",
            "Iteration:  70% 331/475 [03:51<00:58,  2.47it/s]\u001b[A\n",
            "Iteration:  70% 332/475 [03:52<00:57,  2.47it/s]\u001b[A\n",
            "Iteration:  70% 333/475 [03:52<00:57,  2.47it/s]\u001b[A\n",
            "Iteration:  70% 334/475 [03:52<00:57,  2.47it/s]\u001b[A\n",
            "Iteration:  71% 335/475 [03:53<00:56,  2.47it/s]\u001b[A\n",
            "Iteration:  71% 336/475 [03:53<00:56,  2.47it/s]\u001b[A\n",
            "Iteration:  71% 337/475 [03:54<00:55,  2.47it/s]\u001b[A\n",
            "Iteration:  71% 338/475 [03:54<00:55,  2.46it/s]\u001b[A\n",
            "Iteration:  71% 339/475 [03:54<00:55,  2.47it/s]\u001b[A\n",
            "Iteration:  72% 340/475 [03:55<00:54,  2.46it/s]\u001b[A\n",
            "Iteration:  72% 341/475 [03:55<00:54,  2.45it/s]\u001b[A\n",
            "Iteration:  72% 342/475 [03:56<00:53,  2.47it/s]\u001b[A\n",
            "Iteration:  72% 343/475 [03:56<00:53,  2.47it/s]\u001b[A\n",
            "Iteration:  72% 344/475 [03:56<00:53,  2.47it/s]\u001b[A\n",
            "Iteration:  73% 345/475 [03:57<00:52,  2.47it/s]\u001b[A\n",
            "Iteration:  73% 346/475 [03:57<00:52,  2.47it/s]\u001b[A\n",
            "Iteration:  73% 347/475 [03:58<00:52,  2.46it/s]\u001b[A\n",
            "Iteration:  73% 348/475 [03:58<00:51,  2.47it/s]\u001b[A\n",
            "Iteration:  73% 349/475 [03:59<00:51,  2.46it/s]\u001b[A01/07/2020 13:41:56 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-350/config.json\n",
            "01/07/2020 13:41:58 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-350/pytorch_model.bin\n",
            "01/07/2020 13:41:58 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-350\n",
            "01/07/2020 13:42:15 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-350\n",
            "\n",
            "Iteration:  74% 350/475 [04:17<12:20,  5.93s/it]\u001b[A\n",
            "Iteration:  74% 351/475 [04:18<08:49,  4.27s/it]\u001b[A\n",
            "Iteration:  74% 352/475 [04:18<06:22,  3.11s/it]\u001b[A\n",
            "Iteration:  74% 353/475 [04:19<04:40,  2.30s/it]\u001b[A\n",
            "Iteration:  75% 354/475 [04:19<03:29,  1.73s/it]\u001b[A\n",
            "Iteration:  75% 355/475 [04:19<02:40,  1.34s/it]\u001b[A\n",
            "Iteration:  75% 356/475 [04:20<02:05,  1.06s/it]\u001b[A\n",
            "Iteration:  75% 357/475 [04:20<01:41,  1.16it/s]\u001b[A\n",
            "Iteration:  75% 358/475 [04:21<01:24,  1.38it/s]\u001b[A\n",
            "Iteration:  76% 359/475 [04:21<01:12,  1.59it/s]\u001b[A\n",
            "Iteration:  76% 360/475 [04:21<01:04,  1.78it/s]\u001b[A\n",
            "Iteration:  76% 361/475 [04:22<00:58,  1.95it/s]\u001b[A\n",
            "Iteration:  76% 362/475 [04:22<00:54,  2.07it/s]\u001b[A\n",
            "Iteration:  76% 363/475 [04:23<00:51,  2.18it/s]\u001b[A\n",
            "Iteration:  77% 364/475 [04:23<00:49,  2.26it/s]\u001b[A\n",
            "Iteration:  77% 365/475 [04:23<00:47,  2.32it/s]\u001b[A\n",
            "Iteration:  77% 366/475 [04:24<00:46,  2.36it/s]\u001b[A\n",
            "Iteration:  77% 367/475 [04:24<00:45,  2.38it/s]\u001b[A\n",
            "Iteration:  77% 368/475 [04:25<00:44,  2.42it/s]\u001b[A\n",
            "Iteration:  78% 369/475 [04:25<00:43,  2.43it/s]\u001b[A\n",
            "Iteration:  78% 370/475 [04:25<00:42,  2.44it/s]\u001b[A\n",
            "Iteration:  78% 371/475 [04:26<00:42,  2.45it/s]\u001b[A\n",
            "Iteration:  78% 372/475 [04:26<00:42,  2.44it/s]\u001b[A\n",
            "Iteration:  79% 373/475 [04:27<00:41,  2.46it/s]\u001b[A\n",
            "Iteration:  79% 374/475 [04:27<00:41,  2.46it/s]\u001b[A\n",
            "Iteration:  79% 375/475 [04:27<00:40,  2.47it/s]\u001b[A\n",
            "Iteration:  79% 376/475 [04:28<00:40,  2.47it/s]\u001b[A\n",
            "Iteration:  79% 377/475 [04:28<00:39,  2.46it/s]\u001b[A\n",
            "Iteration:  80% 378/475 [04:29<00:39,  2.45it/s]\u001b[A\n",
            "Iteration:  80% 379/475 [04:29<00:38,  2.46it/s]\u001b[A\n",
            "Iteration:  80% 380/475 [04:30<00:38,  2.47it/s]\u001b[A\n",
            "Iteration:  80% 381/475 [04:30<00:38,  2.47it/s]\u001b[A\n",
            "Iteration:  80% 382/475 [04:30<00:37,  2.46it/s]\u001b[A\n",
            "Iteration:  81% 383/475 [04:31<00:37,  2.47it/s]\u001b[A\n",
            "Iteration:  81% 384/475 [04:31<00:36,  2.47it/s]\u001b[A\n",
            "Iteration:  81% 385/475 [04:32<00:36,  2.47it/s]\u001b[A\n",
            "Iteration:  81% 386/475 [04:32<00:36,  2.47it/s]\u001b[A\n",
            "Iteration:  81% 387/475 [04:32<00:35,  2.46it/s]\u001b[A\n",
            "Iteration:  82% 388/475 [04:33<00:35,  2.47it/s]\u001b[A\n",
            "Iteration:  82% 389/475 [04:33<00:34,  2.47it/s]\u001b[A\n",
            "Iteration:  82% 390/475 [04:34<00:34,  2.47it/s]\u001b[A\n",
            "Iteration:  82% 391/475 [04:34<00:33,  2.47it/s]\u001b[A\n",
            "Iteration:  83% 392/475 [04:34<00:33,  2.47it/s]\u001b[A\n",
            "Iteration:  83% 393/475 [04:35<00:33,  2.47it/s]\u001b[A\n",
            "Iteration:  83% 394/475 [04:35<00:32,  2.47it/s]\u001b[A\n",
            "Iteration:  83% 395/475 [04:36<00:32,  2.46it/s]\u001b[A\n",
            "Iteration:  83% 396/475 [04:36<00:32,  2.46it/s]\u001b[A\n",
            "Iteration:  84% 397/475 [04:36<00:31,  2.47it/s]\u001b[A\n",
            "Iteration:  84% 398/475 [04:37<00:31,  2.48it/s]\u001b[A\n",
            "Iteration:  84% 399/475 [04:37<00:30,  2.48it/s]\u001b[A01/07/2020 13:42:35 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-400/config.json\n",
            "01/07/2020 13:42:37 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-400/pytorch_model.bin\n",
            "01/07/2020 13:42:37 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-400\n",
            "01/07/2020 13:42:54 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-400\n",
            "\n",
            "Iteration:  84% 400/475 [04:57<07:43,  6.18s/it]\u001b[A\n",
            "Iteration:  84% 401/475 [04:57<05:29,  4.45s/it]\u001b[A\n",
            "Iteration:  85% 402/475 [04:58<03:56,  3.24s/it]\u001b[A\n",
            "Iteration:  85% 403/475 [04:58<02:51,  2.39s/it]\u001b[A\n",
            "Iteration:  85% 404/475 [04:58<02:07,  1.79s/it]\u001b[A\n",
            "Iteration:  85% 405/475 [04:59<01:36,  1.38s/it]\u001b[A\n",
            "Iteration:  85% 406/475 [04:59<01:14,  1.08s/it]\u001b[A\n",
            "Iteration:  86% 407/475 [05:00<00:59,  1.14it/s]\u001b[A\n",
            "Iteration:  86% 408/475 [05:00<00:49,  1.36it/s]\u001b[A\n",
            "Iteration:  86% 409/475 [05:00<00:42,  1.57it/s]\u001b[A\n",
            "Iteration:  86% 410/475 [05:01<00:36,  1.76it/s]\u001b[A\n",
            "Iteration:  87% 411/475 [05:01<00:33,  1.92it/s]\u001b[A\n",
            "Iteration:  87% 412/475 [05:02<00:30,  2.06it/s]\u001b[A\n",
            "Iteration:  87% 413/475 [05:02<00:28,  2.17it/s]\u001b[A\n",
            "Iteration:  87% 414/475 [05:03<00:27,  2.25it/s]\u001b[A\n",
            "Iteration:  87% 415/475 [05:03<00:25,  2.31it/s]\u001b[A\n",
            "Iteration:  88% 416/475 [05:03<00:25,  2.34it/s]\u001b[A\n",
            "Iteration:  88% 417/475 [05:04<00:24,  2.39it/s]\u001b[A\n",
            "Iteration:  88% 418/475 [05:04<00:23,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 419/475 [05:05<00:23,  2.43it/s]\u001b[A\n",
            "Iteration:  88% 420/475 [05:05<00:22,  2.44it/s]\u001b[A\n",
            "Iteration:  89% 421/475 [05:05<00:22,  2.45it/s]\u001b[A\n",
            "Iteration:  89% 422/475 [05:06<00:21,  2.45it/s]\u001b[A\n",
            "Iteration:  89% 423/475 [05:06<00:21,  2.46it/s]\u001b[A\n",
            "Iteration:  89% 424/475 [05:07<00:20,  2.46it/s]\u001b[A\n",
            "Iteration:  89% 425/475 [05:07<00:20,  2.46it/s]\u001b[A\n",
            "Iteration:  90% 426/475 [05:07<00:19,  2.47it/s]\u001b[A\n",
            "Iteration:  90% 427/475 [05:08<00:19,  2.47it/s]\u001b[A\n",
            "Iteration:  90% 428/475 [05:08<00:19,  2.47it/s]\u001b[A\n",
            "Iteration:  90% 429/475 [05:09<00:18,  2.46it/s]\u001b[A\n",
            "Iteration:  91% 430/475 [05:09<00:18,  2.46it/s]\u001b[A\n",
            "Iteration:  91% 431/475 [05:09<00:17,  2.47it/s]\u001b[A\n",
            "Iteration:  91% 432/475 [05:10<00:17,  2.46it/s]\u001b[A\n",
            "Iteration:  91% 433/475 [05:10<00:16,  2.47it/s]\u001b[A\n",
            "Iteration:  91% 434/475 [05:11<00:16,  2.47it/s]\u001b[A\n",
            "Iteration:  92% 435/475 [05:11<00:16,  2.46it/s]\u001b[A\n",
            "Iteration:  92% 436/475 [05:11<00:15,  2.47it/s]\u001b[A\n",
            "Iteration:  92% 437/475 [05:12<00:15,  2.47it/s]\u001b[A\n",
            "Iteration:  92% 438/475 [05:12<00:14,  2.47it/s]\u001b[A\n",
            "Iteration:  92% 439/475 [05:13<00:14,  2.47it/s]\u001b[A\n",
            "Iteration:  93% 440/475 [05:13<00:14,  2.47it/s]\u001b[A\n",
            "Iteration:  93% 441/475 [05:13<00:13,  2.47it/s]\u001b[A\n",
            "Iteration:  93% 442/475 [05:14<00:13,  2.47it/s]\u001b[A\n",
            "Iteration:  93% 443/475 [05:14<00:12,  2.47it/s]\u001b[A\n",
            "Iteration:  93% 444/475 [05:15<00:12,  2.47it/s]\u001b[A\n",
            "Iteration:  94% 445/475 [05:15<00:12,  2.46it/s]\u001b[A\n",
            "Iteration:  94% 446/475 [05:15<00:11,  2.47it/s]\u001b[A\n",
            "Iteration:  94% 447/475 [05:16<00:11,  2.47it/s]\u001b[A\n",
            "Iteration:  94% 448/475 [05:16<00:10,  2.47it/s]\u001b[A\n",
            "Iteration:  95% 449/475 [05:17<00:10,  2.47it/s]\u001b[A01/07/2020 13:43:14 - INFO - transformers.configuration_utils -   Configuration saved in output/checkpoint-450/config.json\n",
            "01/07/2020 13:43:16 - INFO - transformers.modeling_utils -   Model weights saved in output/checkpoint-450/pytorch_model.bin\n",
            "01/07/2020 13:43:16 - INFO - __main__ -   Saving model checkpoint to output/checkpoint-450\n",
            "01/07/2020 13:43:32 - INFO - __main__ -   Saving optimizer and scheduler states to output/checkpoint-450\n",
            "\n",
            "Iteration:  95% 450/475 [05:34<02:19,  5.57s/it]\u001b[A\n",
            "Iteration:  95% 451/475 [05:35<01:36,  4.02s/it]\u001b[A\n",
            "Iteration:  95% 452/475 [05:35<01:07,  2.93s/it]\u001b[A\n",
            "Iteration:  95% 453/475 [05:36<00:47,  2.18s/it]\u001b[A\n",
            "Iteration:  96% 454/475 [05:36<00:34,  1.65s/it]\u001b[A\n",
            "Iteration:  96% 455/475 [05:36<00:25,  1.27s/it]\u001b[A\n",
            "Iteration:  96% 456/475 [05:37<00:19,  1.01s/it]\u001b[A\n",
            "Iteration:  96% 457/475 [05:37<00:14,  1.20it/s]\u001b[A\n",
            "Iteration:  96% 458/475 [05:38<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 459/475 [05:38<00:09,  1.63it/s]\u001b[A\n",
            "Iteration:  97% 460/475 [05:38<00:08,  1.80it/s]\u001b[A\n",
            "Iteration:  97% 461/475 [05:39<00:07,  1.97it/s]\u001b[A\n",
            "Iteration:  97% 462/475 [05:39<00:06,  2.10it/s]\u001b[A\n",
            "Iteration:  97% 463/475 [05:40<00:05,  2.18it/s]\u001b[A\n",
            "Iteration:  98% 464/475 [05:40<00:04,  2.27it/s]\u001b[A\n",
            "Iteration:  98% 465/475 [05:40<00:04,  2.32it/s]\u001b[A\n",
            "Iteration:  98% 466/475 [05:41<00:03,  2.37it/s]\u001b[A\n",
            "Iteration:  98% 467/475 [05:41<00:03,  2.40it/s]\u001b[A\n",
            "Iteration:  99% 468/475 [05:42<00:02,  2.41it/s]\u001b[A\n",
            "Iteration:  99% 469/475 [05:42<00:02,  2.44it/s]\u001b[A\n",
            "Iteration:  99% 470/475 [05:42<00:02,  2.43it/s]\u001b[A\n",
            "Iteration:  99% 471/475 [05:43<00:01,  2.45it/s]\u001b[A\n",
            "Iteration:  99% 472/475 [05:43<00:01,  2.46it/s]\u001b[A\n",
            "Iteration: 100% 473/475 [05:44<00:00,  2.45it/s]\u001b[A\n",
            "Iteration: 100% 474/475 [05:44<00:00,  2.47it/s]\u001b[A\n",
            "Iteration: 100% 475/475 [05:44<00:00,  2.47it/s]\u001b[A\n",
            "Epoch: 100% 1/1 [05:44<00:00, 344.96s/it]\n",
            "01/07/2020 13:43:42 - INFO - __main__ -    global_step = 475, average loss = 2.341442054196408\n",
            "01/07/2020 13:43:42 - INFO - __main__ -   Saving model checkpoint to output\n",
            "01/07/2020 13:43:42 - INFO - transformers.configuration_utils -   Configuration saved in output/config.json\n",
            "01/07/2020 13:43:44 - INFO - transformers.modeling_utils -   Model weights saved in output/pytorch_model.bin\n",
            "01/07/2020 13:43:45 - INFO - transformers.configuration_utils -   loading configuration file output/config.json\n",
            "01/07/2020 13:43:45 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "01/07/2020 13:43:45 - INFO - transformers.modeling_utils -   loading weights file output/pytorch_model.bin\n",
            "01/07/2020 13:43:48 - INFO - transformers.tokenization_utils -   Model name 'output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output' is a path or url to a directory containing tokenizer files.\n",
            "01/07/2020 13:43:48 - INFO - transformers.tokenization_utils -   loading file output/vocab.json\n",
            "01/07/2020 13:43:48 - INFO - transformers.tokenization_utils -   loading file output/merges.txt\n",
            "01/07/2020 13:43:48 - INFO - transformers.tokenization_utils -   loading file output/added_tokens.json\n",
            "01/07/2020 13:43:48 - INFO - transformers.tokenization_utils -   loading file output/special_tokens_map.json\n",
            "01/07/2020 13:43:48 - INFO - transformers.tokenization_utils -   loading file output/tokenizer_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js7GgxqONdwD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "434b15ae-f978-4023-ec39-dd772936c99f"
      },
      "source": [
        "!python /content/transformers/examples/run_generation.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=/content/output/ \\\n",
        "    --length=100"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/07/2020 13:46:08 - INFO - transformers.tokenization_utils -   Model name '/content/output/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output/' is a path or url to a directory containing tokenizer files.\n",
            "01/07/2020 13:46:08 - INFO - transformers.tokenization_utils -   loading file /content/output/vocab.json\n",
            "01/07/2020 13:46:08 - INFO - transformers.tokenization_utils -   loading file /content/output/merges.txt\n",
            "01/07/2020 13:46:08 - INFO - transformers.tokenization_utils -   loading file /content/output/added_tokens.json\n",
            "01/07/2020 13:46:08 - INFO - transformers.tokenization_utils -   loading file /content/output/special_tokens_map.json\n",
            "01/07/2020 13:46:08 - INFO - transformers.tokenization_utils -   loading file /content/output/tokenizer_config.json\n",
            "01/07/2020 13:46:08 - INFO - transformers.configuration_utils -   loading configuration file /content/output/config.json\n",
            "01/07/2020 13:46:08 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "01/07/2020 13:46:08 - INFO - transformers.modeling_utils -   loading weights file /content/output/pytorch_model.bin\n",
            "an obligation issued by a state or political \n",
            "01/07/2020 13:46:14 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=100, model_name_or_path='/content/output/', model_type='gpt2', n_gpu=1, no_cuda=False, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> an obligation issued by a state or political subdivision of a state or political subdivision of a state, or a federal reserve bank, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a federal savings association, or a!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8qvfo2kNq9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f08d91e4-0a8b-4145-ac21-cf967fe17f78"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input.txt  sample_data\ttransformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04FVYGvMNrpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}