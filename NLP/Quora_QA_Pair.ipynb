{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_QA_Pair.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPmQR9yT+TmhQUMhF4jPRPc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheikmohdimran/Experiments_2020/blob/master/NLP/Quora_QA_Pair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mrGnf0MMPc8",
        "colab_type": "code",
        "outputId": "6d5adbf7-d397-4615-b710-33cec5d1e9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"imrandude\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"0bc5f9f7194b6f77f8990f1d092e3af2\" # key from the json file\n",
        "!kaggle competitions download -c quora-question-pairs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            "\r  0% 0.00/21.2M [00:00<?, ?B/s]\r 24% 5.00M/21.2M [00:00<00:00, 39.8MB/s]\r 94% 20.0M/21.2M [00:00<00:00, 51.2MB/s]\n",
            "\r100% 21.2M/21.2M [00:00<00:00, 84.0MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/4.95M [00:00<?, ?B/s]\n",
            "100% 4.95M/4.95M [00:00<00:00, 80.8MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 97% 168M/173M [00:01<00:00, 142MB/s]\n",
            "100% 173M/173M [00:01<00:00, 143MB/s]\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXcVujqYgDGZ",
        "colab_type": "code",
        "outputId": "1acdff09-1efc-41ca-d785-bd95e84eabc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  sample_submission.csv.zip\ttest.csv.zip  train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd-9QQ9JL_Yd",
        "colab_type": "code",
        "outputId": "aec53581-cb0f-4c83-c921-ef7aa2d1cccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1P7L4O3Qavj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./custom_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCLMfzmvMDMx",
        "colab_type": "code",
        "outputId": "164e3642-02b7-40da-93f1-79e71518d285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "print(\"Start Time {}\".format(datetime.now()))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Start Time 2020-02-06 13:47:17.055014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5suyV3-nPzYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_trained_model = None\n",
        "glove_embedding = 'glove_embedding'\n",
        "glove_path = 'glove.6B.300d.txt'\n",
        "fasttext_embedding = 'fasttext_embedding'\n",
        "fasttext_path = 'wiki-news-300d-1M.vec'\n",
        "pre_train_model_dimension = 300\n",
        "embedding_dim = 300\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1woyvOAP2N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_glove_dict(path):\n",
        "    glove_index = dict()\n",
        "    gl_path = os.path.join('pre-trained-model', path)\n",
        "\n",
        "    file = open(gl_path, encoding=\"utf8\")\n",
        "\n",
        "    for line in file:\n",
        "        splited_line = line.split()\n",
        "        word = splited_line[0]\n",
        "        coeficient = np.array(splited_line[1:], dtype='float32')\n",
        "\n",
        "        glove_index[word] = coeficient\n",
        "\n",
        "    file.close()\n",
        "    return glove_index\n",
        "\n",
        "\n",
        "\n",
        "def get_embedding_matrix(glove_dict, token_items, vocab_size, vector_dim):\n",
        "    marix = np.zeros((vocab_size, vector_dim))\n",
        "\n",
        "    for word, idx in token_items:\n",
        "        #word = lemmatizer.lemmatize(word.lower().strip())\n",
        "        vector = glove_dict.get(word)\n",
        "        if vector is not None:\n",
        "            marix[idx] = vector\n",
        "\n",
        "    return marix\n",
        "\n",
        "\n",
        "\n",
        "def get_glove_matrix(glove_path, token_items, vocab_size, dimension):\n",
        "    glove_dict = generate_glove_dict(glove_path)\n",
        "    embed_matrix = get_embedding_matrix(glove_dict, token_items, vocab_size, dimension)\n",
        "    return embed_matrix\n",
        "\n",
        "\n",
        "def get_fasttext_matrix(glove_path, token_items, vocab_size, dimension):\n",
        "    glove_dict = generate_glove_dict(glove_path)\n",
        "    embed_matrix = get_embedding_matrix(glove_dict, token_items, vocab_size, dimension)\n",
        "    return embed_matrix\n",
        "\n",
        "def save_tokenizer(tokenizer, model_folder):\n",
        "    with open(os.path.join(model_folder,'tokenizer_200_b.pickle'), 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def classification_report1(test_data, test_labels):\n",
        "    test_labels = np.array(test_label)\n",
        "    question1 = test_data['question1']\n",
        "    question2 = test_data['question2']\n",
        "    sequences1 = loaded_tokenizer.texts_to_sequences(question1)\n",
        "    sequences2 = loaded_tokenizer.texts_to_sequences(question2)\n",
        "    padded_sequences1 = pad_sequences(sequences1, maxlen =20, padding='post')\n",
        "    padded_sequences2 = pad_sequences(sequences2, maxlen =20, padding='post')\n",
        "    test_predicted = model.predict([padded_sequences1, padded_sequences2])\n",
        "    predicted_labels = np.round(test_predicted)\n",
        "    print(predicted_labels)\n",
        "    predicted_labels = [ int(label[0]) for label in predicted_labels]\n",
        "    print(\"Classification report ========> \")\n",
        "    print(classification_report(test_labels, predicted_labels))\n",
        "    print(\"Accuracy score ======> \")\n",
        "    print(accuracy_score(test_labels, predicted_labels))\n",
        "    return True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCKJeywyrl0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add tf.keras.layers.InputLayer --> \n",
        "# https://stackoverflow.com/questions/45217973/what-is-the-advantage-of-using-an-inputlayer-or-an-input-in-a-keras-model-with\n",
        "\n",
        "def get_model(vocab_size, embedding_dim, embedding_matrix, input_length):\n",
        "    model = tf.keras.Sequential([\n",
        "\t  tf.keras.layers.InputLayer(input_shape=input_length*2),\n",
        "    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim, weights=[embedding_matrix],input_length = input_length*2,trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(30)),\n",
        "    tf.keras.layers.Dense(30,activation = 'relu'),\n",
        "    tf.keras.layers.Dense(20,activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na8HOxftP6uN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('train.csv',skiprows=1,header=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOeygdSUP7wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df=df.iloc[:,3:5].values\n",
        "train_labels=df.iloc[:,5:6].values\n",
        "epochs = 5\n",
        "batch_size = 1000\n",
        "maxlen = 20\n",
        "input_length = 20\n",
        "n_split=2\n",
        "i=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEhKZFhWP9bx",
        "colab_type": "code",
        "outputId": "eefb5727-41e8-495a-bab7-cca1b416d2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index,test_index in KFold(n_split).split(train_df):\n",
        "  print(\"Preprocessing + Training + Plotting + Test data preprocessing + Evaluation + Prediction + Reporting Start Time {}\".format(datetime.now()))\n",
        "  print(\"Pre-Processing Start Time {}\".format(datetime.now()))\n",
        "  x_train,x_test=train_df[train_index],train_df[test_index]\n",
        "  y_train,y_test=train_labels[train_index],train_labels[test_index]\n",
        "\n",
        "\n",
        "  question1, question2 = [], []\n",
        "  for each in x_train:#['question1'].to_string(index=False)\n",
        "      question1.append(str(each[0]))\n",
        "      question2.append(str(each[1]))\n",
        "\n",
        "  tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "  tokenizer.fit_on_texts(question1+question2)\n",
        "  vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "  sequences1 = tokenizer.texts_to_sequences(question1)\n",
        "  sequences2 = tokenizer.texts_to_sequences(question2)\n",
        "\n",
        "  padded_sequences1 = pad_sequences(sequences1, maxlen =maxlen, padding='post')\n",
        "  padded_sequences2 = pad_sequences(sequences2, maxlen =maxlen, padding='post')\n",
        "  padded_sequences = np.concatenate((padded_sequences1, padded_sequences2),axis=1)\n",
        "\n",
        "  if pre_trained_model == 'glove':\n",
        "      model_folder = glove_embedding\n",
        "      embedding_matrix = get_glove_matrix(glove_path, tokenizer.word_index.items(), vocab_size, pre_train_model_dimension)\n",
        "  elif pre_trained_model == 'fasttext':\n",
        "      model_folder = fasttext_embedding\n",
        "      embedding_matrix = get_fasttext_matrix(fasttext_path, tokenizer.word_index.items(), vocab_size, pre_train_model_dimension)\n",
        "  else:\n",
        "      model_folder = './custom_embedding'\n",
        "      embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "  with open(os.path.join(model_folder,'tokenizer_200_b_kfold_'+str(i)+'.pickle'), 'wb') as handle:\n",
        "      pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  print(\"Pre-Processing End Time {}\".format(datetime.now()))\n",
        "  print(\"Model Training Start Time {}\".format(datetime.now()))\n",
        "  model=get_model(vocab_size, embedding_dim, embedding_matrix, input_length)\n",
        "  model.compile(loss='binary_crossentropy', optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001,decay = 1e-6, momentum=0.9), metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "  # This callback will stop the training when there is no improvement in\n",
        "  #the validation loss for three consecutive epochs.\n",
        "\n",
        "  history = model.fit(padded_sequences,#[padded_sequences1, padded_sequences2], \n",
        "                      y_train, \n",
        "                      callbacks=[callback], batch_size=batch_size, \n",
        "                      epochs = epochs, validation_split=0.2)\n",
        "  #history = model.fit([padded_sequences1, padded_sequences2], y_train, epochs = epochs, steps_per_epoch=5, batch_size=batch_size)\n",
        "  print(\"Model Training End Time {}\".format(datetime.now()))\n",
        "  model.save(os.path.join(model_folder,'model_200_b_kfold_'+str(i)+'.h5'))\n",
        "\n",
        "  i +=1\n",
        "  print(\"Model Created\")\n",
        "  print(\"<==============================================================>\")\n",
        "  print(\"Prediction/Evaluation Started\")\n",
        "  print(\"Test Data Pre-processing Start Time {}\".format(datetime.now()))\n",
        "  question1, question2 = [], []\n",
        "  for each in x_test:#['question1'].to_string(index=False)\n",
        "      question1.append(str(each[0]))\n",
        "      question2.append(str(each[1]))\n",
        "  \n",
        "  sequences1 = tokenizer.texts_to_sequences(question1)\n",
        "  sequences2 = tokenizer.texts_to_sequences(question2)\n",
        "\n",
        "  padded_sequences1 = pad_sequences(sequences1, maxlen =20, padding='post')\n",
        "  padded_sequences2 = pad_sequences(sequences2, maxlen =20, padding='post')\n",
        "  padded_sequences = np.concatenate((padded_sequences1, padded_sequences2),axis=1)\n",
        "\n",
        "  print(\"Test Data Pre-processing End Time {}\".format(datetime.now()))\n",
        "  #print('Model evaluation ',model.evaluate(padded_sequences,y_test))\n",
        "  \n",
        "  print(\"Test Data Prediction Start Time {}\".format(datetime.now()))\n",
        "  #y_test_predicted = model.predict([padded_sequences1, padded_sequences2], steps=5)\n",
        "  y_test_predicted = model.predict(padded_sequences)#, steps=5)\n",
        "  print(\"Test Data Prediction End Time {}\".format(datetime.now()))\n",
        "\n",
        "  print(\"Preprocessing + Training + Plotting + Test data preprocessing + Evaluation + Prediction + Reporting End Time {}\".format(datetime.now()))\n",
        "  print(\"Prediction/Evaluation End\")\n",
        "  print(\"<==================================================================================================================>\")\n",
        "print(\"End Time {}\".format(datetime.now()))\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing + Training + Plotting + Test data preprocessing + Evaluation + Prediction + Reporting Start Time 2020-02-06 14:30:37.708317\n",
            "Pre-Processing Start Time 2020-02-06 14:30:37.708811\n",
            "Pre-Processing End Time 2020-02-06 14:30:55.682226\n",
            "Model Training Start Time 2020-02-06 14:30:55.682421\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 40, 300)           20874600  \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 60)                79440     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 30)                1830      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 20,956,511\n",
            "Trainable params: 81,911\n",
            "Non-trainable params: 20,874,600\n",
            "_________________________________________________________________\n",
            "Train on 161716 samples, validate on 40429 samples\n",
            "Epoch 1/5\n",
            "161716/161716 [==============================] - 77s 477us/sample - loss: 0.6636 - acc: 0.6296 - val_loss: 0.6471 - val_acc: 0.6515\n",
            "Epoch 2/5\n",
            "161716/161716 [==============================] - 74s 461us/sample - loss: 0.6592 - acc: 0.6296 - val_loss: 0.6477 - val_acc: 0.6515\n",
            "Epoch 3/5\n",
            "161716/161716 [==============================] - 75s 461us/sample - loss: 0.6592 - acc: 0.6296 - val_loss: 0.6478 - val_acc: 0.6515\n",
            "Epoch 4/5\n",
            "161716/161716 [==============================] - 78s 482us/sample - loss: 0.6592 - acc: 0.6296 - val_loss: 0.6475 - val_acc: 0.6515\n",
            "Epoch 5/5\n",
            "161716/161716 [==============================] - 74s 455us/sample - loss: 0.6592 - acc: 0.6296 - val_loss: 0.6474 - val_acc: 0.6515\n",
            "Model Training End Time 2020-02-06 14:37:21.306780\n",
            "Model Created\n",
            "<==============================================================>\n",
            "Prediction/Evaluation Started\n",
            "Test Data Pre-processing Start Time 2020-02-06 14:37:25.983999\n",
            "Test Data Pre-processing End Time 2020-02-06 14:37:35.395880\n",
            "Test Data Prediction Start Time 2020-02-06 14:37:35.396343\n",
            "Test Data Prediction End Time 2020-02-06 14:38:52.762429\n",
            "Preprocessing + Training + Plotting + Test data preprocessing + Evaluation + Prediction + Reporting End Time 2020-02-06 14:38:52.762592\n",
            "Prediction/Evaluation End\n",
            "<==================================================================================================================>\n",
            "Preprocessing + Training + Plotting + Test data preprocessing + Evaluation + Prediction + Reporting Start Time 2020-02-06 14:38:52.765916\n",
            "Pre-Processing Start Time 2020-02-06 14:38:52.766053\n",
            "Pre-Processing End Time 2020-02-06 14:39:10.603165\n",
            "Model Training Start Time 2020-02-06 14:39:10.603412\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 40, 300)           20742600  \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 60)                79440     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 30)                1830      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 20,824,511\n",
            "Trainable params: 81,911\n",
            "Non-trainable params: 20,742,600\n",
            "_________________________________________________________________\n",
            "Train on 161716 samples, validate on 40429 samples\n",
            "Epoch 1/5\n",
            "161716/161716 [==============================] - 79s 486us/sample - loss: 0.6643 - acc: 0.6276 - val_loss: 0.6601 - val_acc: 0.6279\n",
            "Epoch 2/5\n",
            "161716/161716 [==============================] - 74s 456us/sample - loss: 0.6603 - acc: 0.6276 - val_loss: 0.6601 - val_acc: 0.6279\n",
            "Epoch 3/5\n",
            "161716/161716 [==============================] - 75s 462us/sample - loss: 0.6603 - acc: 0.6276 - val_loss: 0.6601 - val_acc: 0.6279\n",
            "Epoch 4/5\n",
            "161716/161716 [==============================] - 78s 485us/sample - loss: 0.6603 - acc: 0.6276 - val_loss: 0.6601 - val_acc: 0.6279\n",
            "Epoch 5/5\n",
            "161716/161716 [==============================] - 80s 494us/sample - loss: 0.6603 - acc: 0.6276 - val_loss: 0.6603 - val_acc: 0.6279\n",
            "Model Training End Time 2020-02-06 14:45:43.715087\n",
            "Model Created\n",
            "<==============================================================>\n",
            "Prediction/Evaluation Started\n",
            "Test Data Pre-processing Start Time 2020-02-06 14:45:49.223422\n",
            "Test Data Pre-processing End Time 2020-02-06 14:45:58.723719\n",
            "Test Data Prediction Start Time 2020-02-06 14:45:58.723920\n",
            "Test Data Prediction End Time 2020-02-06 14:47:18.431046\n",
            "Preprocessing + Training + Plotting + Test data preprocessing + Evaluation + Prediction + Reporting End Time 2020-02-06 14:47:18.431228\n",
            "Prediction/Evaluation End\n",
            "<==================================================================================================================>\n",
            "End Time 2020-02-06 14:47:18.432955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "483Xh8yQ31tf",
        "colab_type": "text"
      },
      "source": [
        "## Inference Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW68aKXW33sY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "57d86573-aac1-41b7-d898-b0442c714058"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "K.set_learning_phase(0)\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('./custom_embedding/model_200_b_kfold_1.h5')\n",
        "print(model.inputs)\n",
        "print(model.outputs)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "[<tf.Tensor 'embedding_6_input:0' shape=(?, 40) dtype=float32>]\n",
            "[<tf.Tensor 'dense_20/Sigmoid:0' shape=(?, 1) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig_x99sP4OtT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "4d6c2946-03ca-491b-b9a3-d1ef17f3e4ee"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        print(tf.global_variables())\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        # Graph -> GraphDef ProtoBuf\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
        "                                                      output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "\n",
        "\n",
        "frozen_graph = freeze_session(K.get_session(),output_names=[out.op.name for out in model.outputs])\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'embedding_6/embeddings:0' shape=(69582, 300) dtype=float32>, <tf.Variable 'bidirectional_6/forward_lstm_6/kernel:0' shape=(300, 120) dtype=float32>, <tf.Variable 'bidirectional_6/forward_lstm_6/recurrent_kernel:0' shape=(30, 120) dtype=float32>, <tf.Variable 'bidirectional_6/forward_lstm_6/bias:0' shape=(120,) dtype=float32>, <tf.Variable 'bidirectional_6/backward_lstm_6/kernel:0' shape=(300, 120) dtype=float32>, <tf.Variable 'bidirectional_6/backward_lstm_6/recurrent_kernel:0' shape=(30, 120) dtype=float32>, <tf.Variable 'bidirectional_6/backward_lstm_6/bias:0' shape=(120,) dtype=float32>, <tf.Variable 'dense_18/kernel:0' shape=(60, 30) dtype=float32>, <tf.Variable 'dense_18/bias:0' shape=(30,) dtype=float32>, <tf.Variable 'dense_19/kernel:0' shape=(30, 20) dtype=float32>, <tf.Variable 'dense_19/bias:0' shape=(20,) dtype=float32>, <tf.Variable 'dense_20/kernel:0' shape=(20, 1) dtype=float32>, <tf.Variable 'dense_20/bias:0' shape=(1,) dtype=float32>, <tf.Variable 'training/RMSprop/iter:0' shape=() dtype=int64>, <tf.Variable 'training/RMSprop/decay:0' shape=() dtype=float32>, <tf.Variable 'training/RMSprop/learning_rate:0' shape=() dtype=float32>, <tf.Variable 'training/RMSprop/momentum:0' shape=() dtype=float32>, <tf.Variable 'training/RMSprop/rho:0' shape=() dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/forward_lstm_6/kernel/rms:0' shape=(300, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/forward_lstm_6/recurrent_kernel/rms:0' shape=(30, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/forward_lstm_6/bias/rms:0' shape=(120,) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/backward_lstm_6/kernel/rms:0' shape=(300, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/backward_lstm_6/recurrent_kernel/rms:0' shape=(30, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/backward_lstm_6/bias/rms:0' shape=(120,) dtype=float32>, <tf.Variable 'training/RMSprop/dense_18/kernel/rms:0' shape=(60, 30) dtype=float32>, <tf.Variable 'training/RMSprop/dense_18/bias/rms:0' shape=(30,) dtype=float32>, <tf.Variable 'training/RMSprop/dense_19/kernel/rms:0' shape=(30, 20) dtype=float32>, <tf.Variable 'training/RMSprop/dense_19/bias/rms:0' shape=(20,) dtype=float32>, <tf.Variable 'training/RMSprop/dense_20/kernel/rms:0' shape=(20, 1) dtype=float32>, <tf.Variable 'training/RMSprop/dense_20/bias/rms:0' shape=(1,) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/forward_lstm_6/kernel/momentum:0' shape=(300, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/forward_lstm_6/recurrent_kernel/momentum:0' shape=(30, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/forward_lstm_6/bias/momentum:0' shape=(120,) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/backward_lstm_6/kernel/momentum:0' shape=(300, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/backward_lstm_6/recurrent_kernel/momentum:0' shape=(30, 120) dtype=float32>, <tf.Variable 'training/RMSprop/bidirectional_6/backward_lstm_6/bias/momentum:0' shape=(120,) dtype=float32>, <tf.Variable 'training/RMSprop/dense_18/kernel/momentum:0' shape=(60, 30) dtype=float32>, <tf.Variable 'training/RMSprop/dense_18/bias/momentum:0' shape=(30,) dtype=float32>, <tf.Variable 'training/RMSprop/dense_19/kernel/momentum:0' shape=(30, 20) dtype=float32>, <tf.Variable 'training/RMSprop/dense_19/bias/momentum:0' shape=(20,) dtype=float32>, <tf.Variable 'training/RMSprop/dense_20/kernel/momentum:0' shape=(20, 1) dtype=float32>, <tf.Variable 'training/RMSprop/dense_20/bias/momentum:0' shape=(1,) dtype=float32>]\n",
            "WARNING:tensorflow:From <ipython-input-2-77b19505f1b1>:18: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 42 variables.\n",
            "INFO:tensorflow:Converted 42 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbIbEgJq4TSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "affb4d2d-54e2-411c-b0df-35ab0d70dc70"
      },
      "source": [
        "tf.train.write_graph(frozen_graph, \"./custom_embedding/\", \"model_200_b_kfold_1.pb\", as_text=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./custom_embedding/model_200_b_kfold_1.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPsn6KHN4nIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "655580fd-b604-4ecc-d220-24e9a1f58e20"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "f = gfile.FastGFile(\"./custom_embedding/model_200_b_kfold_1.pb\", 'rb')\n",
        "graph_def = tf.GraphDef()\n",
        "graph_def.ParseFromString(f.read())\n",
        "f.close()\n",
        "\n",
        "sess = K.get_session()\n",
        "tf.import_graph_def(graph_def)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-f772d13a4e0b>:4: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.gfile.GFile.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L74EefiQ4w_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a313d68e-ef23-49c8-e30f-57cff133e346"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open(os.path.join('./custom_embedding', 'tokenizer_200_b_kfold_1.pickle'), 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n",
        "\n",
        "df = pd.read_csv('./test.csv')\n",
        "df = df.dropna()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4fh6d0T46cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "02c2b583-439c-4eb1-e180-7a047deb26e4"
      },
      "source": [
        "def get_microseconds(s_date, e_date):\n",
        "    diff = e_date-s_date\n",
        "    microseconds = 1000000 * diff.seconds\n",
        "    microseconds = microseconds + diff.microseconds\n",
        "    return microseconds\n",
        "\n",
        "def microsecond_to_seconds(microseconds):\n",
        "    seconds = microseconds / 1000000\n",
        "    return seconds\n",
        "\n",
        "sigmoid_tensor = sess.graph.get_tensor_by_name('import/dense_20/Sigmoid:0')\n",
        "processing_records = 200000\n",
        "inference_time = 0\n",
        "def inference_report(test_data):\n",
        "    question1 = test_data['question1']\n",
        "    question2 = test_data['question2']\n",
        "    sequences1 = loaded_tokenizer.texts_to_sequences(question1)\n",
        "    sequences2 = loaded_tokenizer.texts_to_sequences(question2)\n",
        "    padded_sequences1 = pad_sequences(sequences1, maxlen =20, padding='post')\n",
        "    padded_sequences2 = pad_sequences(sequences2, maxlen =20, padding='post')\n",
        "    padded_sequences = np.concatenate((padded_sequences1, padded_sequences2), axis=1)\n",
        "    #padded_sequences1 = tf.convert_to_tensor(padded_sequences1)\n",
        "    #padded_sequences2 = tf.convert_to_tensor(padded_sequences2)    \n",
        "    predict = model.predict(padded_sequences)#[padded_sequences1, padded_sequences2], steps=5)\n",
        "    print(predict)\n",
        "    #aa = np.concatenate((tf.convert_to_tensor(padded_sequences1), tf.convert_to_tensor(padded_sequences2)), axis=None)\n",
        "    start_inf = datetime.now()\n",
        "    predictions = sess.run(sigmoid_tensor, {'import/embedding_6_input:0': np.concatenate((padded_sequences1, padded_sequences2), axis=1)})\n",
        "    #predictions = sess.run(sigmoid_tensor, {'import/embedding_input:0': [padded_sequences1, padded_sequences2]})\n",
        "    end_inf = datetime.now()\n",
        "    inference_time = get_microseconds(start_inf, end_inf)\n",
        "    \n",
        "    print(\"Total Inference Time {}(microseconds) for {} Records\".format(inference_time, processing_records))\n",
        "    avg_inf_time = float(microsecond_to_seconds(inference_time)/processing_records)\n",
        "    print(\"Average Inference Time {} for One record\".format(avg_inf_time))\n",
        "    \n",
        "inference_report(df[0:processing_records])    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.36885345]\n",
            " [0.36885345]\n",
            " [0.36885345]\n",
            " ...\n",
            " [0.36885345]\n",
            " [0.36885345]\n",
            " [0.36885345]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk9qpZU05ezW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}